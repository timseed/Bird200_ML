{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from glob import glob\n",
    "from random import randint\n",
    "DIR_WHERE_IMAGES_ARE=\"data/trimmed/*/*\"\n",
    "#In this case the classes are the 3rd level\n",
    "\n",
    "def train_split_dataframes(dpath):\n",
    "    df=pd.DataFrame.from_records([[g] for g in glob(DIR_WHERE_IMAGES_ARE)],columns=['file'])\n",
    "\n",
    "    df['Class']=df.file.apply(lambda x: x.split('/')[2])\n",
    "    df['Test']=False\n",
    "    df['Train']=False\n",
    "\n",
    "    df_cnt=df.groupby('Class').size().to_frame()\n",
    "    df_cnt.columns=['Cnt']\n",
    "\n",
    "    df=df.merge(df_cnt,on='Class')\n",
    "\n",
    "    def set_train(max_occurences_in_class):\n",
    "        #We want 66% to be training data\n",
    "        r=randint(1,int(max_occurences_in_class))\n",
    "        if r/max_occurences_in_class <= .61:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    df.Train = df.apply(lambda row: set_train(row['Cnt']),axis=1)\n",
    "    df.Test = ~df.Train\n",
    "    return df\n",
    "\n",
    "df=train_split_dataframes(DIR_WHERE_IMAGES_ARE)\n",
    "CLASSES=len(set(list(df.Class)))\n",
    "print(\"Check Percentage {}\".format(len(df[df.Test==True])/len(df[df.Test==False])))\n",
    "\n",
    "df_test=df[df.Test==True]\n",
    "df_train=df[df.Train==True]\n",
    "\n",
    "# there should be non in both data sets\n",
    "df_test.merge(df_train,on='file')\n",
    "\n",
    "from keras.models import Sequential\n",
    "#Import from keras_preprocessing not from keras.preprocessing\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "\n",
    "SIZE_X = SIZE_Y = 70\n",
    "\n",
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "# Train and Valid both come from TRAIN\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "x_col=\"file\",\n",
    "y_col=\"Class\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(SIZE_X,SIZE_Y))\n",
    "\n",
    "valid_generator=datagen.flow_from_dataframe(\n",
    "dataframe=df_train,\n",
    "x_col=\"file\",\n",
    "y_col=\"Class\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(SIZE_X,SIZE_Y))\n",
    "\n",
    "# Test comes from Test\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=df_test,\n",
    "x_col=\"file\",\n",
    "y_col=\"Class\",\n",
    "batch_size=32,\n",
    "seed=42,\n",
    "shuffle=True,\n",
    "class_mode=\"categorical\",\n",
    "target_size=(SIZE_X,SIZE_Y))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(SIZE_X,SIZE_Y,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(SIZE_X, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(CLASSES, activation='softmax'))\n",
    "model.compile(optimizers.rmsprop(lr=0.0001, decay=1e-6),loss=\"categorical_crossentropy\",metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "callbacks_wanted = [\n",
    "  # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
    "  tf.keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "  # Write TensorBoard logs to `./logs` directory\n",
    "  tf.keras.callbacks.TensorBoard(log_dir='./logs', \n",
    "                                 histogram_freq=0,\n",
    "                            write_graph=True, write_images=True)\n",
    "]\n",
    "\n",
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    callbacks=callbacks_wanted,\n",
    "                    epochs=10\n",
    ")\n",
    "\n",
    "model.save(\"birds.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Saves the model to a single HDF5 file.\n",
       "\n",
       "The savefile includes:\n",
       "    - The model architecture, allowing to re-instantiate the model.\n",
       "    - The model weights.\n",
       "    - The state of the optimizer, allowing to resume training\n",
       "        exactly where you left off.\n",
       "\n",
       "This allows you to save the entirety of the state of a model\n",
       "in a single file.\n",
       "\n",
       "Saved models can be reinstantiated via `keras.models.load_model`.\n",
       "The model returned by `load_model`\n",
       "is a compiled model ready to be used (unless the saved model\n",
       "was never compiled in the first place).\n",
       "\n",
       "# Arguments\n",
       "    filepath: String, path to the file to save the weights to.\n",
       "    overwrite: Whether to silently overwrite any existing file at the\n",
       "        target location, or provide the user with a manual prompt.\n",
       "    include_optimizer: If True, save optimizer's state together.\n",
       "\n",
       "# Example\n",
       "\n",
       "```python\n",
       "from keras.models import load_model\n",
       "\n",
       "model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
       "del model  # deletes the existing model\n",
       "\n",
       "# returns a compiled model\n",
       "# identical to the previous one\n",
       "model = load_model('my_model.h5')\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m      ~/pe37/lib/python3.7/site-packages/keras/engine/network.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.save?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
